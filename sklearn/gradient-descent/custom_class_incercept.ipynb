{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca07f82f",
   "metadata": {},
   "source": [
    "# Custom class\n",
    "> to calculate b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adfc7727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b866bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde8870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = make_regression(n_samples=100, n_features=1, n_informative=1, n_targets=1, noise = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa81dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x12a1dbee0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y,color = 'deepskyblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a08ef34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LinearRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using sklearn to fetch actual results\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c25df33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54.04010894]\n",
      "0.25202525314812885\n"
     ]
    }
   ],
   "source": [
    "print(lr.coef_)\n",
    "print(lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74269fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDRegressor:\n",
    "    def __init__(self, learning_rate, epochs):\n",
    "        self.m = 54.04010894\n",
    "        self.b = -500    # random value\n",
    "        self.n = learning_rate\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        # calculating b using GD\n",
    "        for i in range(self.epochs):\n",
    "            loss_slope = (-2/100) * np.sum(y - self.m*x.ravel() - self.b)\n",
    "            self.b = self.b - (loss_slope * self.n)\n",
    "            print(f\" step{i} | loss: {loss_slope} | b: {self.b}\")\n",
    "        print(self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26acf617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step0 | loss: -1000.5040505071825 | b: -489.9949594949282\n",
      " step1 | loss: -980.4939694970388 | b: -480.1900197999578\n",
      " step2 | loss: -960.8840901070981 | b: -470.5811788988868\n",
      " step3 | loss: -941.6664083049559 | b: -461.1645148158372\n",
      " step4 | loss: -922.8330801388569 | b: -451.93618401444866\n",
      " step5 | loss: -904.3764185360798 | b: -442.8924198290879\n",
      " step6 | loss: -886.2888901653582 | b: -434.0295309274343\n",
      " step7 | loss: -868.5631123620509 | b: -425.34389980381377\n",
      " step8 | loss: -851.1918501148098 | b: -416.83198130266567\n",
      " step9 | loss: -834.1680131125136 | b: -408.4903011715405\n",
      " step10 | loss: -817.4846528502635 | b: -400.31545464303787\n",
      " step11 | loss: -801.1349597932581 | b: -392.3041050451053\n",
      " step12 | loss: -785.1122605973927 | b: -384.45298243913135\n",
      " step13 | loss: -769.4100153854451 | b: -376.7588822852769\n",
      " step14 | loss: -754.0218150777363 | b: -369.21866413449953\n",
      " step15 | loss: -738.9413787761814 | b: -361.8292503467377\n",
      " step16 | loss: -724.1625512006577 | b: -354.5876248347311\n",
      " step17 | loss: -709.6793001766445 | b: -347.49083183296466\n",
      " step18 | loss: -695.4857141731117 | b: -340.53597469123355\n",
      " step19 | loss: -681.5759998896496 | b: -333.72021469233704\n",
      " step20 | loss: -667.9444798918565 | b: -327.0407698934185\n",
      " step21 | loss: -654.5855902940193 | b: -320.49491399047827\n",
      " step22 | loss: -641.493878488139 | b: -314.0799752055969\n",
      " step23 | loss: -628.6640009183762 | b: -307.7933351964131\n",
      " step24 | loss: -616.0907209000086 | b: -301.63242798741305\n",
      " step25 | loss: -603.7689064820086 | b: -295.59473892259297\n",
      " step26 | loss: -591.6935283523683 | b: -289.6778036390693\n",
      " step27 | loss: -579.8596577853209 | b: -283.87920706121605\n",
      " step28 | loss: -568.2624646296144 | b: -278.19658241491993\n",
      " step29 | loss: -556.8972153370223 | b: -272.6276102615497\n",
      " step30 | loss: -545.7592710302819 | b: -267.1700175512469\n",
      " step31 | loss: -534.8440856096761 | b: -261.8215766951501\n",
      " step32 | loss: -524.1472038974827 | b: -256.58010465617525\n",
      " step33 | loss: -513.664259819533 | b: -251.44346205797993\n",
      " step34 | loss: -503.3909746231422 | b: -246.4095523117485\n",
      " step35 | loss: -493.3231551306795 | b: -241.47632076044172\n",
      " step36 | loss: -483.45669202806585 | b: -236.64175384016107\n",
      " step37 | loss: -473.78755818750454 | b: -231.90387825828603\n",
      " step38 | loss: -464.31180702375445 | b: -227.2607601880485\n",
      " step39 | loss: -455.0255708832794 | b: -222.7105044792157\n",
      " step40 | loss: -445.9250594656138 | b: -218.25125388455956\n",
      " step41 | loss: -437.00655827630146 | b: -213.88118830179656\n",
      " step42 | loss: -428.26642711077557 | b: -209.5985240306888\n",
      " step43 | loss: -419.70109856856 | b: -205.40151304500318\n",
      " step44 | loss: -411.3070765971888 | b: -201.2884422790313\n",
      " step45 | loss: -403.080935065245 | b: -197.25763292837885\n",
      " step46 | loss: -395.01931636394016 | b: -193.30743976473946\n",
      " step47 | loss: -387.1189300366614 | b: -189.43625046437285\n",
      " step48 | loss: -379.37655143592804 | b: -185.64248495001357\n",
      " step49 | loss: -371.78902040720953 | b: -181.92459474594148\n",
      " step50 | loss: -364.35323999906547 | b: -178.28106234595083\n",
      " step51 | loss: -357.0661751990841 | b: -174.71040059395997\n",
      " step52 | loss: -349.92485169510235 | b: -171.21115207700896\n",
      " step53 | loss: -342.92635466120026 | b: -167.78188853039697\n",
      " step54 | loss: -336.06782756797634 | b: -164.4212102547172\n",
      " step55 | loss: -329.3464710166167 | b: -161.12774554455103\n",
      " step56 | loss: -322.75954159628446 | b: -157.9001501285882\n",
      " step57 | loss: -316.3043507643588 | b: -154.73710662094462\n",
      " step58 | loss: -309.9782637490716 | b: -151.6373239834539\n",
      " step59 | loss: -303.77869847409016 | b: -148.599536998713\n",
      " step60 | loss: -297.7031245046084 | b: -145.62250575366693\n",
      " step61 | loss: -291.74906201451626 | b: -142.70501513352175\n",
      " step62 | loss: -285.9140807742259 | b: -139.8458743257795\n",
      " step63 | loss: -280.1957991587414 | b: -137.0439163341921\n",
      " step64 | loss: -274.5918831755666 | b: -134.29799750243643\n",
      " step65 | loss: -269.1000455120552 | b: -131.60699704731587\n",
      " step66 | loss: -263.71804460181414 | b: -128.96981660129774\n",
      " step67 | loss: -258.4436837097778 | b: -126.38537976419995\n",
      " step68 | loss: -253.27481003558228 | b: -123.85263166384414\n",
      " step69 | loss: -248.20931383487067 | b: -121.37053852549543\n",
      " step70 | loss: -243.24512755817324 | b: -118.9380872499137\n",
      " step71 | loss: -238.3802250070098 | b: -116.5542849998436\n",
      " step72 | loss: -233.61262050686958 | b: -114.21815879477491\n",
      " step73 | loss: -228.94036809673221 | b: -111.92875511380758\n",
      " step74 | loss: -224.36156073479754 | b: -109.6851395064596\n",
      " step75 | loss: -219.8743295201016 | b: -107.4863962112586\n",
      " step76 | loss: -215.47684292969956 | b: -105.3316277819616\n",
      " step77 | loss: -211.1673060711056 | b: -103.21995472125055\n",
      " step78 | loss: -206.9439599496835 | b: -101.15051512175371\n",
      " step79 | loss: -202.80508075068985 | b: -99.12246431424681\n",
      " step80 | loss: -198.748979135676 | b: -97.13497452289005\n",
      " step81 | loss: -194.7739995529625 | b: -95.18723452736042\n",
      " step82 | loss: -190.8785195619032 | b: -93.27844933174138\n",
      " step83 | loss: -187.0609491706651 | b: -91.40783984003474\n",
      " step84 | loss: -183.31973018725188 | b: -89.57464253816222\n",
      " step85 | loss: -179.65333558350682 | b: -87.77810918232716\n",
      " step86 | loss: -176.06026887183668 | b: -86.01750649360879\n",
      " step87 | loss: -172.53906349439995 | b: -84.29211585866479\n",
      " step88 | loss: -169.08828222451197 | b: -82.60123303641967\n",
      " step89 | loss: -165.70651658002174 | b: -80.94416787061945\n",
      " step90 | loss: -162.39238624842127 | b: -79.32024400813523\n",
      " step91 | loss: -159.14453852345284 | b: -77.72879862290071\n",
      " step92 | loss: -155.9616477529838 | b: -76.16918214537087\n",
      " step93 | loss: -152.84241479792414 | b: -74.64075799739163\n",
      " step94 | loss: -149.78556650196563 | b: -73.14290233237197\n",
      " step95 | loss: -146.78985517192635 | b: -71.67500378065272\n",
      " step96 | loss: -143.8540580684878 | b: -70.23646319996784\n",
      " step97 | loss: -140.9769769071181 | b: -68.82669343089667\n",
      " step98 | loss: -138.1574373689757 | b: -67.44511905720691\n",
      " step99 | loss: -135.39428862159622 | b: -66.09117617099095\n",
      " step100 | loss: -132.6864028491643 | b: -64.7643121424993\n",
      " step101 | loss: -130.032674792181 | b: -63.4639853945775\n",
      " step102 | loss: -127.43202129633737 | b: -62.189665181614124\n",
      " step103 | loss: -124.88338087041066 | b: -60.94083137291002\n",
      " step104 | loss: -122.38571325300244 | b: -59.71697424038\n",
      " step105 | loss: -119.93799898794238 | b: -58.51759425050057\n",
      " step106 | loss: -117.53923900818354 | b: -57.34220186041874\n",
      " step107 | loss: -115.18845422801988 | b: -56.19031731813854\n",
      " step108 | loss: -112.88468514345945 | b: -55.061470466703945\n",
      " step109 | loss: -110.62699144059026 | b: -53.955200552298045\n",
      " step110 | loss: -108.41445161177847 | b: -52.87105603618026\n",
      " step111 | loss: -106.24616257954291 | b: -51.808594410384835\n",
      " step112 | loss: -104.12123932795207 | b: -50.76738201710531\n",
      " step113 | loss: -102.03881454139298 | b: -49.74699387169138\n",
      " step114 | loss: -99.99803825056516 | b: -48.747013489185726\n",
      " step115 | loss: -97.99807748555384 | b: -47.767032714330185\n",
      " step116 | loss: -96.03811593584274 | b: -46.80665155497176\n",
      " step117 | loss: -94.11735361712591 | b: -45.8654780188005\n",
      " step118 | loss: -92.23500654478339 | b: -44.943127953352665\n",
      " step119 | loss: -90.39030641388773 | b: -44.039224889213784\n",
      " step120 | loss: -88.58250028560995 | b: -43.15339988635768\n",
      " step121 | loss: -86.81085027989776 | b: -42.28529138355871\n",
      " step122 | loss: -85.0746332742998 | b: -41.43454505081571\n",
      " step123 | loss: -83.3731406088138 | b: -40.600813644727566\n",
      " step124 | loss: -81.70567779663752 | b: -39.78375686676119\n",
      " step125 | loss: -80.07156424070476 | b: -38.98304122435414\n",
      " step126 | loss: -78.47013295589066 | b: -38.19833989479523\n",
      " step127 | loss: -76.90073029677285 | b: -37.4293325918275\n",
      " step128 | loss: -75.36271569083739 | b: -36.67570543491913\n",
      " step129 | loss: -73.85546137702065 | b: -35.937150821148926\n",
      " step130 | loss: -72.37835214948024 | b: -35.213367299654124\n",
      " step131 | loss: -70.93078510649062 | b: -34.50405944858922\n",
      " step132 | loss: -69.51216940436082 | b: -33.80893775454561\n",
      " step133 | loss: -68.1219260162736 | b: -33.127718494382876\n",
      " step134 | loss: -66.75948749594815 | b: -32.46012361942339\n",
      " step135 | loss: -65.42429774602917 | b: -31.8058806419631\n",
      " step136 | loss: -64.1158117911086 | b: -31.164722524052014\n",
      " step137 | loss: -62.83349555528641 | b: -30.53638756849915\n",
      " step138 | loss: -61.57682564418068 | b: -29.920619312057344\n",
      " step139 | loss: -60.34528913129706 | b: -29.317166420744375\n",
      " step140 | loss: -59.138383348671134 | b: -28.72578258725766\n",
      " step141 | loss: -57.9556156816977 | b: -28.146226430440684\n",
      " step142 | loss: -56.79650336806375 | b: -27.57826139676005\n",
      " step143 | loss: -55.66057330070248 | b: -27.021655663753023\n",
      " step144 | loss: -54.54736183468843 | b: -26.476182045406137\n",
      " step145 | loss: -53.456414597994666 | b: -25.94161789942619\n",
      " step146 | loss: -52.38728630603476 | b: -25.417745036365844\n",
      " step147 | loss: -51.339540579914065 | b: -24.904349630566703\n",
      " step148 | loss: -50.3127497683158 | b: -24.401222132883543\n",
      " step149 | loss: -49.30649477294947 | b: -23.90815718515405\n",
      " step150 | loss: -48.32036487749048 | b: -23.424953536379146\n",
      " step151 | loss: -47.35395757994067 | b: -22.95141396057974\n",
      " step152 | loss: -46.406878428341855 | b: -22.48734517629632\n",
      " step153 | loss: -45.47874085977501 | b: -22.03255776769857\n",
      " step154 | loss: -44.569166042579525 | b: -21.586866107272776\n",
      " step155 | loss: -43.67778272172794 | b: -21.150088280055495\n",
      " step156 | loss: -42.80422706729337 | b: -20.722046009382563\n",
      " step157 | loss: -41.9481425259475 | b: -20.302564584123086\n",
      " step158 | loss: -41.10917967542855 | b: -19.8914727873688\n",
      " step159 | loss: -40.28699608191999 | b: -19.488602826549602\n",
      " step160 | loss: -39.48125616028158 | b: -19.093790264946787\n",
      " step161 | loss: -38.69163103707596 | b: -18.706873954576025\n",
      " step162 | loss: -37.917798416334435 | b: -18.32769597041268\n",
      " step163 | loss: -37.15944244800774 | b: -17.956101545932604\n",
      " step164 | loss: -36.416253599047586 | b: -17.591939009942127\n",
      " step165 | loss: -35.68792852706663 | b: -17.235059724671462\n",
      " step166 | loss: -34.9741699565253 | b: -16.88531802510621\n",
      " step167 | loss: -34.274686557394794 | b: -16.54257115953226\n",
      " step168 | loss: -33.58919282624691 | b: -16.206679231269792\n",
      " step169 | loss: -32.91740896972196 | b: -15.877505141572572\n",
      " step170 | loss: -32.259060790327524 | b: -15.554914533669296\n",
      " step171 | loss: -31.613879574520972 | b: -15.238775737924087\n",
      " step172 | loss: -30.98160198303055 | b: -14.92895971809378\n",
      " step173 | loss: -30.361969943369946 | b: -14.625340018660081\n",
      " step174 | loss: -29.754730544502547 | b: -14.327792713215056\n",
      " step175 | loss: -29.159635933612496 | b: -14.03619635387893\n",
      " step176 | loss: -28.57644321494024 | b: -13.750431921729527\n",
      " step177 | loss: -28.004914350641435 | b: -13.470382778223113\n",
      " step178 | loss: -27.44481606362861 | b: -13.195934617586827\n",
      " step179 | loss: -26.895919742356032 | b: -12.926975420163267\n",
      " step180 | loss: -26.358001347508907 | b: -12.663395406688178\n",
      " step181 | loss: -25.830841320558733 | b: -12.405086993482591\n",
      " step182 | loss: -25.314224494147563 | b: -12.151944748541116\n",
      " step183 | loss: -24.807940004264612 | b: -11.903865348498469\n",
      " step184 | loss: -24.31178120417932 | b: -11.660747536456675\n",
      " step185 | loss: -23.82554558009573 | b: -11.422492080655719\n",
      " step186 | loss: -23.34903466849381 | b: -11.18900173397078\n",
      " step187 | loss: -22.88205397512394 | b: -10.96018119421954\n",
      " step188 | loss: -22.424412895621458 | b: -10.735937065263325\n",
      " step189 | loss: -21.97592463770903 | b: -10.516177818886234\n",
      " step190 | loss: -21.536406144954846 | b: -10.300813757436686\n",
      " step191 | loss: -21.10567802205575 | b: -10.089756977216128\n",
      " step192 | loss: -20.68356446161463 | b: -9.882921332599981\n",
      " step193 | loss: -20.26989317238234 | b: -9.680222400876158\n",
      " step194 | loss: -19.864495308934693 | b: -9.48157744778681\n",
      " step195 | loss: -19.467205402755997 | b: -9.28690539375925\n",
      " step196 | loss: -19.077861294700877 | b: -9.096126780812241\n",
      " step197 | loss: -18.696304068806864 | b: -8.909163740124173\n",
      " step198 | loss: -18.32237798743072 | b: -8.725939960249866\n",
      " step199 | loss: -17.955930427682112 | b: -8.546380655973044\n",
      " step200 | loss: -17.59681181912847 | b: -8.370412537781759\n",
      " step201 | loss: -17.244875582745898 | b: -8.1979637819543\n",
      " step202 | loss: -16.899978071090977 | b: -8.02896400124339\n",
      " step203 | loss: -16.561978509669157 | b: -7.863344216146698\n",
      " step204 | loss: -16.230738939475778 | b: -7.701036826751941\n",
      " step205 | loss: -15.906124160686261 | b: -7.541975585145078\n",
      " step206 | loss: -15.588001677472537 | b: -7.386095568370353\n",
      " step207 | loss: -15.276241643923084 | b: -7.233333151931122\n",
      " step208 | loss: -14.970716811044623 | b: -7.083625983820676\n",
      " step209 | loss: -14.67130247482373 | b: -6.936912959072439\n",
      " step210 | loss: -14.377876425327255 | b: -6.793134194819166\n",
      " step211 | loss: -14.090318896820714 | b: -6.652231005850959\n",
      " step212 | loss: -13.808512518884296 | b: -6.514145880662117\n",
      " step213 | loss: -13.53234226850661 | b: -6.3788224579770505\n",
      " step214 | loss: -13.26169542313648 | b: -6.246205503745686\n",
      " step215 | loss: -12.996461514673749 | b: -6.1162408885989485\n",
      " step216 | loss: -12.736532284380276 | b: -5.988875565755146\n",
      " step217 | loss: -12.48180163869267 | b: -5.864057549368219\n",
      " step218 | loss: -12.232165605918818 | b: -5.7417358933090314\n",
      " step219 | loss: -11.987522293800442 | b: -5.621860670371027\n",
      " step220 | loss: -11.747771847924435 | b: -5.504382951891783\n",
      " step221 | loss: -11.512816410965945 | b: -5.389254787782123\n",
      " step222 | loss: -11.282560082746626 | b: -5.276429186954657\n",
      " step223 | loss: -11.056908881091696 | b: -5.165860098143741\n",
      " step224 | loss: -10.83577070346986 | b: -5.0575023911090415\n",
      " step225 | loss: -10.61905528940046 | b: -4.9513118382150365\n",
      " step226 | loss: -10.406674183612452 | b: -4.847245096378912\n",
      " step227 | loss: -10.198540699940205 | b: -4.74525968937951\n",
      " step228 | loss: -9.994569885941397 | b: -4.645313990520096\n",
      " step229 | loss: -9.79467848822257 | b: -4.547367205637871\n",
      " step230 | loss: -9.598784918458119 | b: -4.451379356453289\n",
      " step231 | loss: -9.406809220088956 | b: -4.3573112642524\n",
      " step232 | loss: -9.21867303568718 | b: -4.265124533895528\n",
      " step233 | loss: -9.034299574973435 | b: -4.174781538145794\n",
      " step234 | loss: -8.853613583473967 | b: -4.086245402311055\n",
      " step235 | loss: -8.676541311804488 | b: -3.9994799891930097\n",
      " step236 | loss: -8.5030104855684 | b: -3.914449884337326\n",
      " step237 | loss: -8.332950275857032 | b: -3.8311203815787556\n",
      " step238 | loss: -8.166291270339892 | b: -3.749457468875357\n",
      " step239 | loss: -8.002965444933093 | b: -3.669427814426026\n",
      " step240 | loss: -7.842906136034433 | b: -3.5909987530656817\n",
      " step241 | loss: -7.686048013313746 | b: -3.514138272932544\n",
      " step242 | loss: -7.532327053047467 | b: -3.4388150024020696\n",
      " step243 | loss: -7.381680511986517 | b: -3.3649981972822043\n",
      " step244 | loss: -7.234046901746788 | b: -3.2926577282647362\n",
      " step245 | loss: -7.089365963711851 | b: -3.2217640686276177\n",
      " step246 | loss: -6.947578644437615 | b: -3.1522882821832416\n",
      " step247 | loss: -6.808627071548864 | b: -3.084202011467753\n",
      " step248 | loss: -6.672454530117884 | b: -3.017477466166574\n",
      " step249 | loss: -6.539005439515529 | b: -2.952087411771419\n",
      " step250 | loss: -6.408225330725216 | b: -2.888005158464167\n",
      " step251 | loss: -6.2800608241107145 | b: -2.82520455022306\n",
      " step252 | loss: -6.1544596076285005 | b: -2.763659954146775\n",
      " step253 | loss: -6.031370415475927 | b: -2.703346249992016\n",
      " step254 | loss: -5.910743007166412 | b: -2.6442388199203517\n",
      " step255 | loss: -5.792528147023084 | b: -2.586313538450121\n",
      " step256 | loss: -5.67667758408262 | b: -2.5295467626092947\n",
      " step257 | loss: -5.563144032400966 | b: -2.473915322285285\n",
      " step258 | loss: -5.451881151752952 | b: -2.4193965107677555\n",
      " step259 | loss: -5.342843528717889 | b: -2.3659680754805765\n",
      " step260 | loss: -5.235986658143532 | b: -2.313608208899141\n",
      " step261 | loss: -5.131266924980659 | b: -2.2622955396493345\n",
      " step262 | loss: -5.028641586481048 | b: -2.212009123784524\n",
      " step263 | loss: -4.928068754751429 | b: -2.1627284362370096\n",
      " step264 | loss: -4.829507379656398 | b: -2.1144333624404457\n",
      " step265 | loss: -4.732917232063267 | b: -2.067104190119813\n",
      " step266 | loss: -4.638258887422006 | b: -2.020721601245593\n",
      " step267 | loss: -4.545493709673565 | b: -1.9752666641488572\n",
      " step268 | loss: -4.4545838354800935 | b: -1.9307208257940562\n",
      " step269 | loss: -4.36549215877049 | b: -1.8870659042063513\n",
      " step270 | loss: -4.278182315595083 | b: -1.8442840810504004\n",
      " step271 | loss: -4.192618669283181 | b: -1.8023578943575687\n",
      " step272 | loss: -4.108766295897518 | b: -1.7612702313985935\n",
      " step273 | loss: -4.026590969979567 | b: -1.7210043216987978\n",
      " step274 | loss: -3.9460591505799756 | b: -1.681543730192998\n",
      " step275 | loss: -3.867137967568376 | b: -1.6428723505173144\n",
      " step276 | loss: -3.7897952082170088 | b: -1.6049743984351443\n",
      " step277 | loss: -3.713999304052669 | b: -1.5678344053946176\n",
      " step278 | loss: -3.6397193179716156 | b: -1.5314372122149014\n",
      " step279 | loss: -3.566924931612181 | b: -1.4957679628987797\n",
      " step280 | loss: -3.4955864329799398 | b: -1.4608120985689803\n",
      " step281 | loss: -3.4256747043203384 | b: -1.426555351525777\n",
      " step282 | loss: -3.3571612102339343 | b: -1.3929837394234377\n",
      " step283 | loss: -3.2900179860292535 | b: -1.360083559563145\n",
      " step284 | loss: -3.224217626308668 | b: -1.3278413833000584\n",
      " step285 | loss: -3.159733273782496 | b: -1.2962440505622335\n",
      " step286 | loss: -3.0965386083068474 | b: -1.265278664479165\n",
      " step287 | loss: -3.0346078361407085 | b: -1.2349325861177578\n",
      " step288 | loss: -2.973915679417894 | b: -1.2051934293235789\n",
      " step289 | loss: -2.914437365829536 | b: -1.1760490556652836\n",
      " step290 | loss: -2.856148618512946 | b: -1.147487569480154\n",
      " step291 | loss: -2.7990256461426886 | b: -1.1194973130187271\n",
      " step292 | loss: -2.7430451332198316 | b: -1.0920668616865288\n",
      " step293 | loss: -2.688184230555436 | b: -1.0651850193809744\n",
      " step294 | loss: -2.634420545944329 | b: -1.038840813921531\n",
      " step295 | loss: -2.581732135025441 | b: -1.0130234925712767\n",
      " step296 | loss: -2.530097492324934 | b: -0.9877225176480273\n",
      " step297 | loss: -2.4794955424784324 | b: -0.9629275622232429\n",
      " step298 | loss: -2.429905631628864 | b: -0.9386285059069543\n",
      " step299 | loss: -2.3813075189962882 | b: -0.9148154307169915\n",
      " step300 | loss: -2.3336813686163618 | b: -0.8914786170308279\n",
      " step301 | loss: -2.287007741244034 | b: -0.8686085396183876\n",
      " step302 | loss: -2.2412675864191547 | b: -0.846195863754196\n",
      " step303 | loss: -2.1964422346907715 | b: -0.8242314414072882\n",
      " step304 | loss: -2.152513389996955 | b: -0.8027063075073186\n",
      " step305 | loss: -2.1094631221970177 | b: -0.7816116762853484\n",
      " step306 | loss: -2.0672738597530773 | b: -0.7609389376878176\n",
      " step307 | loss: -2.0259283825580154 | b: -0.7406796538622374\n",
      " step308 | loss: -1.9854098149068535 | b: -0.7208255557131689\n",
      " step309 | loss: -1.9457016186087175 | b: -0.7013685395270817\n",
      " step310 | loss: -1.9067875862365413 | b: -0.6823006636647162\n",
      " step311 | loss: -1.8686518345118124 | b: -0.6636141453195981\n",
      " step312 | loss: -1.8312787978215763 | b: -0.6453013573413823\n",
      " step313 | loss: -1.7946532218651425 | b: -0.6273548251227309\n",
      " step314 | loss: -1.7587601574278398 | b: -0.6097672235484525\n",
      " step315 | loss: -1.723584954279284 | b: -0.5925313740056596\n",
      " step316 | loss: -1.6891132551936978 | b: -0.5756402414537226\n",
      " step317 | loss: -1.6553309900898239 | b: -0.5590869315528244\n",
      " step318 | loss: -1.6222243702880286 | b: -0.5428646878499441\n",
      " step319 | loss: -1.5897798828822667 | b: -0.5269668890211214\n",
      " step320 | loss: -1.5579842852246228 | b: -0.5113870461688752\n",
      " step321 | loss: -1.5268245995201308 | b: -0.4961188001736739\n",
      " step322 | loss: -1.4962881075297276 | b: -0.4811559190983766\n",
      " step323 | loss: -1.4663623453791332 | b: -0.4664922956445853\n",
      " step324 | loss: -1.4370350984715488 | b: -0.45212194465986977\n",
      " step325 | loss: -1.408294396502119 | b: -0.4380390006948486\n",
      " step326 | loss: -1.380128508572078 | b: -0.4242377156091278\n",
      " step327 | loss: -1.3525259384006347 | b: -0.4107124562251215\n",
      " step328 | loss: -1.3254754196326202 | b: -0.39745770202879527\n",
      " step329 | loss: -1.2989659112399694 | b: -0.3844680429163956\n",
      " step330 | loss: -1.2729865930151707 | b: -0.3717381769862439\n",
      " step331 | loss: -1.247526861154865 | b: -0.35926290837469527\n",
      " step332 | loss: -1.22257632393177 | b: -0.3470371451353776\n",
      " step333 | loss: -1.1981247974531324 | b: -0.33505589716084627\n",
      " step334 | loss: -1.1741623015040719 | b: -0.32331427414580555\n",
      " step335 | loss: -1.1506790554739903 | b: -0.31180748359106564\n",
      " step336 | loss: -1.1276654743645107 | b: -0.30053082884742055\n",
      " step337 | loss: -1.1051121648772209 | b: -0.28947970719864835\n",
      " step338 | loss: -1.0830099215796767 | b: -0.2786496079828516\n",
      " step339 | loss: -1.0613497231480815 | b: -0.2680361107513708\n",
      " step340 | loss: -1.0401227286851193 | b: -0.2576348834645196\n",
      " step341 | loss: -1.019320274111417 | b: -0.24744168072340544\n",
      " step342 | loss: -0.9989338686291893 | b: -0.23745234203711355\n",
      " step343 | loss: -0.9789551912566071 | b: -0.2276627901245475\n",
      " step344 | loss: -0.9593760874314745 | b: -0.21806902925023275\n",
      " step345 | loss: -0.9401885656828437 | b: -0.20866714359340433\n",
      " step346 | loss: -0.9213847943691871 | b: -0.19945329564971245\n",
      " step347 | loss: -0.9029570984818025 | b: -0.19042372466489443\n",
      " step348 | loss: -0.8848979565121701 | b: -0.18157474509977273\n",
      " step349 | loss: -0.8671999973819253 | b: -0.17290274512595347\n",
      " step350 | loss: -0.8498559974342843 | b: -0.16440418515161062\n",
      " step351 | loss: -0.8328588774856006 | b: -0.1560755963767546\n",
      " step352 | loss: -0.8162016999358881 | b: -0.14791357937739574\n",
      " step353 | loss: -0.7998776659371692 | b: -0.13991480271802406\n",
      " step354 | loss: -0.7838801126184279 | b: -0.13207600159183977\n",
      " step355 | loss: -0.7682025103660598 | b: -0.12439397648817917\n",
      " step356 | loss: -0.7528384601587367 | b: -0.1168655918865918\n",
      " step357 | loss: -0.7377816909555622 | b: -0.10948777497703618\n",
      " step358 | loss: -0.7230260571364512 | b: -0.10225751440567167\n",
      " step359 | loss: -0.7085655359937221 | b: -0.09517185904573446\n",
      " step360 | loss: -0.6943942252738482 | b: -0.08822791679299598\n",
      " step361 | loss: -0.6805063407683702 | b: -0.08142285338531227\n",
      " step362 | loss: -0.6668962139530037 | b: -0.07475389124578223\n",
      " step363 | loss: -0.6535582896739424 | b: -0.0682183083490428\n",
      " step364 | loss: -0.6404871238804651 | b: -0.06181343711023815\n",
      " step365 | loss: -0.6276773814028558 | b: -0.05553666329620959\n",
      " step366 | loss: -0.6151238337747972 | b: -0.04938542495846162\n",
      " step367 | loss: -0.6028213570993015 | b: -0.043357211387468605\n",
      " step368 | loss: -0.5907649299573161 | b: -0.03744956208789545\n",
      " step369 | loss: -0.5789496313581703 | b: -0.03166006577431374\n",
      " step370 | loss: -0.5673706387310066 | b: -0.025986359387003673\n",
      " step371 | loss: -0.5560232259563884 | b: -0.02042612712743979\n",
      " step372 | loss: -0.5449027614372588 | b: -0.014977099513067202\n",
      " step373 | loss: -0.5340047062085147 | b: -0.009637052450982054\n",
      " step374 | loss: -0.5233246120843452 | b: -0.004403806330138602\n",
      " step375 | loss: -0.5128581198426565 | b: 0.0007247748682879633\n",
      " step376 | loss: -0.5026009574458021 | b: 0.005750784442745985\n",
      " step377 | loss: -0.49254893829688684 | b: 0.010676273825714854\n",
      " step378 | loss: -0.48269795953094785 | b: 0.015503253421024332\n",
      " step379 | loss: -0.4730440003403315 | b: 0.020233693424427646\n",
      " step380 | loss: -0.46358312033352295 | b: 0.024869524627762877\n",
      " step381 | loss: -0.4543114579268531 | b: 0.02941263920703141\n",
      " step382 | loss: -0.44522522876831644 | b: 0.033864891494714576\n",
      " step383 | loss: -0.4363207241929486 | b: 0.03822809873664406\n",
      " step384 | loss: -0.4275943097090897 | b: 0.042504041833734955\n",
      " step385 | loss: -0.4190424235149082 | b: 0.04669446606888404\n",
      " step386 | loss: -0.4106615750446095 | b: 0.050801081819330136\n",
      " step387 | loss: -0.4024483435437189 | b: 0.054825565254767325\n",
      " step388 | loss: -0.39439937667284525 | b: 0.058769559021495776\n",
      " step389 | loss: -0.3865113891393861 | b: 0.06263467291288964\n",
      " step390 | loss: -0.37878116135659895 | b: 0.06642248452645563\n",
      " step391 | loss: -0.37120553812946794 | b: 0.07013453990775032\n",
      " step392 | loss: -0.3637814273668775 | b: 0.07377235418141909\n",
      " step393 | loss: -0.35650579881954075 | b: 0.0773374121696145\n",
      " step394 | loss: -0.3493756828431502 | b: 0.080831168998046\n",
      " step395 | loss: -0.3423881691862873 | b: 0.08425505068990888\n",
      " step396 | loss: -0.33554040580256145 | b: 0.0876104547479345\n",
      " step397 | loss: -0.32882959768651077 | b: 0.09089875072479961\n",
      " step398 | loss: -0.3222530057327792 | b: 0.0941212807821274\n",
      " step399 | loss: -0.3158079456181254 | b: 0.09727936023830866\n",
      " step400 | loss: -0.3094917867057619 | b: 0.10037427810536628\n",
      " step401 | loss: -0.30330195097164747 | b: 0.10340729761508276\n",
      " step402 | loss: -0.29723591195221344 | b: 0.1063796567346049\n",
      " step403 | loss: -0.2912911937131713 | b: 0.1092925686717366\n",
      " step404 | loss: -0.2854653698389062 | b: 0.11214722237012567\n",
      " step405 | loss: -0.279756062442127 | b: 0.11494478299454694\n",
      " step406 | loss: -0.27416094119328277 | b: 0.11768639240647977\n",
      " step407 | loss: -0.2686777223694179 | b: 0.12037316963017394\n",
      " step408 | loss: -0.26330416792203065 | b: 0.12300621130939425\n",
      " step409 | loss: -0.25803808456358934 | b: 0.12558659215503015\n",
      " step410 | loss: -0.25287732287231707 | b: 0.12811536538375332\n",
      " step411 | loss: -0.24781977641487202 | b: 0.13059356314790205\n",
      " step412 | loss: -0.24286338088657503 | b: 0.13302219695676779\n",
      " step413 | loss: -0.23800611326884266 | b: 0.13540225808945622\n",
      " step414 | loss: -0.23324599100346619 | b: 0.13773471799949089\n",
      " step415 | loss: -0.22858107118339688 | b: 0.14002052871132487\n",
      " step416 | loss: -0.22400944975972878 | b: 0.14226062320892216\n",
      " step417 | loss: -0.2195292607645348 | b: 0.1444559158165675\n",
      " step418 | loss: -0.21513867554924473 | b: 0.14660730257205995\n",
      " step419 | loss: -0.21083590203825836 | b: 0.14871566159244254\n",
      " step420 | loss: -0.20661918399749296 | b: 0.15078185343241746\n",
      " step421 | loss: -0.20248680031754362 | b: 0.1528067214355929\n",
      " step422 | loss: -0.19843706431119273 | b: 0.15479109207870484\n",
      " step423 | loss: -0.19446832302497072 | b: 0.15673577530895455\n",
      " step424 | loss: -0.19057895656446974 | b: 0.15864156487459924\n",
      " step425 | loss: -0.18676737743317973 | b: 0.16050923864893105\n",
      " step426 | loss: -0.18303202988451858 | b: 0.16233955894777624\n",
      " step427 | loss: -0.17937138928682628 | b: 0.1641332728406445\n",
      " step428 | loss: -0.175783961501091 | b: 0.1658911124556554\n",
      " step429 | loss: -0.17226828227107008 | b: 0.16761379527836612\n",
      " step430 | loss: -0.16882291662564627 | b: 0.16930202444462258\n",
      " step431 | loss: -0.1654464582931334 | b: 0.17095648902755392\n",
      " step432 | loss: -0.1621375291272713 | b: 0.17257786431882663\n",
      " step433 | loss: -0.1588947785447283 | b: 0.1741668121042739\n",
      " step434 | loss: -0.1557168829738305 | b: 0.1757239809340122\n",
      " step435 | loss: -0.1526025453143555 | b: 0.17725000638715577\n",
      " step436 | loss: -0.14955049440806875 | b: 0.17874551133123645\n",
      " step437 | loss: -0.14655948451990625 | b: 0.18021110617643551\n",
      " step438 | loss: -0.14362829482950787 | b: 0.1816473891247306\n",
      " step439 | loss: -0.14075572893291594 | b: 0.18305494641405975\n",
      " step440 | loss: -0.13794061435425875 | b: 0.18443435255760235\n",
      " step441 | loss: -0.13518180206717476 | b: 0.1857861705782741\n",
      " step442 | loss: -0.13247816602582943 | b: 0.18711095223853239\n",
      " step443 | loss: -0.12982860270531232 | b: 0.1884092382655855\n",
      " step444 | loss: -0.12723203065120842 | b: 0.1896815585720976\n",
      " step445 | loss: -0.12468739003818413 | b: 0.19092843247247943\n",
      " step446 | loss: -0.1221936422374202 | b: 0.19215036889485362\n",
      " step447 | loss: -0.11974976939267236 | b: 0.19334786658878034\n",
      " step448 | loss: -0.11735477400481827 | b: 0.19452141432882852\n",
      " step449 | loss: -0.11500767852472193 | b: 0.19567149111407575\n",
      " step450 | loss: -0.11270752495422656 | b: 0.196798566363618\n",
      " step451 | loss: -0.11045337445514236 | b: 0.19790310010816942\n",
      " step452 | loss: -0.10824430696603855 | b: 0.1989855431778298\n",
      " step453 | loss: -0.10607942082672053 | b: 0.200046337386097\n",
      " step454 | loss: -0.1039578324101842 | b: 0.20108591571019885\n",
      " step455 | loss: -0.10187867576198102 | b: 0.20210470246781867\n",
      " step456 | loss: -0.09984110224674084 | b: 0.2031031134902861\n",
      " step457 | loss: -0.09784428020180698 | b: 0.20408155629230415\n",
      " step458 | loss: -0.09588739459777046 | b: 0.20504043023828186\n",
      " step459 | loss: -0.09396964670581412 | b: 0.20598012670534\n",
      " step460 | loss: -0.09209025377169723 | b: 0.20690102924305698\n",
      " step461 | loss: -0.09024844869626648 | b: 0.20780351373001965\n",
      " step462 | loss: -0.0884434797223389 | b: 0.20868794852724304\n",
      " step463 | loss: -0.08667461012789417 | b: 0.20955469462852197\n",
      " step464 | loss: -0.0849411179253343 | b: 0.21040410580777533\n",
      " step465 | loss: -0.08324229556682823 | b: 0.21123652876344362\n",
      " step466 | loss: -0.08157744965549213 | b: 0.21205230325999855\n",
      " step467 | loss: -0.07994590066238175 | b: 0.21285176226662236\n",
      " step468 | loss: -0.07834698264913299 | b: 0.2136352320931137\n",
      " step469 | loss: -0.07678004299615196 | b: 0.21440303252307522\n",
      " step470 | loss: -0.07524444213622858 | b: 0.2151554769444375\n",
      " step471 | loss: -0.07373955329350239 | b: 0.21589287247737254\n",
      " step472 | loss: -0.07226476222763381 | b: 0.21661552009964888\n",
      " step473 | loss: -0.07081946698308031 | b: 0.21732371476947968\n",
      " step474 | loss: -0.06940307764341952 | b: 0.21801774554591388\n",
      " step475 | loss: -0.06801501609055094 | b: 0.21869789570681938\n",
      " step476 | loss: -0.06665471576874225 | b: 0.21936444286450682\n",
      " step477 | loss: -0.06532162145336486 | b: 0.22001765907904047\n",
      " step478 | loss: -0.06401518902429743 | b: 0.22065781096928344\n",
      " step479 | loss: -0.0627348852438118 | b: 0.22128515982172156\n",
      " step480 | loss: -0.06148018753893695 | b: 0.22189996169711093\n",
      " step481 | loss: -0.060250583788156294 | b: 0.2225024675349925\n",
      " step482 | loss: -0.05904557211239194 | b: 0.22309292325611643\n",
      " step483 | loss: -0.057864660670144306 | b: 0.2236715698628179\n",
      " step484 | loss: -0.05670736745674304 | b: 0.22423864353738532\n",
      " step485 | loss: -0.05557322010760785 | b: 0.2247943757384614\n",
      " step486 | loss: -0.054461755705456055 | b: 0.22533899329551596\n",
      " step487 | loss: -0.05337252059134741 | b: 0.22587271850142943\n",
      " step488 | loss: -0.05230507017952178 | b: 0.22639576920322466\n",
      " step489 | loss: -0.05125896877593022 | b: 0.22690835889098396\n",
      " step490 | loss: -0.05023378940041198 | b: 0.22741069678498807\n",
      " step491 | loss: -0.049229113612402675 | b: 0.2279029879211121\n",
      " step492 | loss: -0.04824453134015428 | b: 0.22838543323451366\n",
      " step493 | loss: -0.047279640713350946 | b: 0.22885822964164718\n",
      " step494 | loss: -0.04633404789908496 | b: 0.22932157012063803\n",
      " step495 | loss: -0.04540736694110237 | b: 0.22977564379004906\n",
      " step496 | loss: -0.0444992196022811 | b: 0.23022063598607187\n",
      " step497 | loss: -0.04360923521023537 | b: 0.23065672833817422\n",
      " step498 | loss: -0.042737050506029564 | b: 0.23108409884323453\n",
      " step499 | loss: -0.041882309495910874 | b: 0.23150292193819363\n",
      " step500 | loss: -0.041044663305991096 | b: 0.23191336857125355\n",
      " step501 | loss: -0.04022377003987145 | b: 0.23231560627165226\n",
      " step502 | loss: -0.03941929463907428 | b: 0.232709799218043\n",
      " step503 | loss: -0.0386309087462935 | b: 0.23309610830550592\n",
      " step504 | loss: -0.037858290571367664 | b: 0.2334746912112196\n",
      " step505 | loss: -0.0371011247599391 | b: 0.233845702458819\n",
      " step506 | loss: -0.03635910226473953 | b: 0.2342092934814664\n",
      " step507 | loss: -0.03563192021944616 | b: 0.23456561268366086\n",
      " step508 | loss: -0.03491928181505827 | b: 0.23491480550181143\n",
      " step509 | loss: -0.03422089617875528 | b: 0.23525701446359898\n",
      " step510 | loss: -0.03353647825518081 | b: 0.23559237924615078\n",
      " step511 | loss: -0.03286574869007808 | b: 0.23592103673305156\n",
      " step512 | loss: -0.03220843371627439 | b: 0.2362431210702143\n",
      " step513 | loss: -0.031564265041949396 | b: 0.2365587637206338\n",
      " step514 | loss: -0.03093297974111209 | b: 0.23686809351804494\n",
      " step515 | loss: -0.03031432014628905 | b: 0.23717123671950782\n",
      " step516 | loss: -0.02970803374336441 | b: 0.23746831705694146\n",
      " step517 | loss: -0.029113873068495905 | b: 0.23775945578762642\n",
      " step518 | loss: -0.02853159560712797 | b: 0.2380447717436977\n",
      " step519 | loss: -0.02796096369498329 | b: 0.23832438138064754\n",
      " step520 | loss: -0.02740174442108298 | b: 0.23859839882485837\n",
      " step521 | loss: -0.02685370953266215 | b: 0.238866935920185\n",
      " step522 | loss: -0.02631663534200783 | b: 0.23913010227360507\n",
      " step523 | loss: -0.02579030263516728 | b: 0.23938800529995674\n",
      " step524 | loss: -0.025274496582464393 | b: 0.23964075026578138\n",
      " step525 | loss: -0.024769006650817573 | b: 0.23988844033228957\n",
      " step526 | loss: -0.024273626517800153 | b: 0.24013117659746758\n",
      " step527 | loss: -0.023788153987443224 | b: 0.24036905813734202\n",
      " step528 | loss: -0.02331239090769415 | b: 0.24060218204641898\n",
      " step529 | loss: -0.022846143089541613 | b: 0.2408306434773144\n",
      " step530 | loss: -0.022389220227750712 | b: 0.2410545356795919\n",
      " step531 | loss: -0.021941435823194126 | b: 0.24127395003782384\n",
      " step532 | loss: -0.021502607106733507 | b: 0.24148897610889117\n",
      " step533 | loss: -0.021072554964597714 | b: 0.24169970165853716\n",
      " step534 | loss: -0.020651103865304776 | b: 0.2419062126971902\n",
      " step535 | loss: -0.020238081787998966 | b: 0.2421085935150702\n",
      " step536 | loss: -0.01983332015223894 | b: 0.24230692671659257\n",
      " step537 | loss: -0.019436653749191634 | b: 0.24250129325408448\n",
      " step538 | loss: -0.01904792067421013 | b: 0.24269177246082657\n",
      " step539 | loss: -0.01866696226072662 | b: 0.24287844208343384\n",
      " step540 | loss: -0.01829362301551129 | b: 0.24306137831358895\n",
      " step541 | loss: -0.017927750555201242 | b: 0.24324065581914098\n",
      " step542 | loss: -0.017569195544097197 | b: 0.24341634777458196\n",
      " step543 | loss: -0.01721781163321481 | b: 0.24358852589091412\n",
      " step544 | loss: -0.016873455400551195 | b: 0.24375726044491963\n",
      " step545 | loss: -0.01653598629253821 | b: 0.243922620307845\n",
      " step546 | loss: -0.01620526656669 | b: 0.2440846729735119\n",
      " step547 | loss: -0.015881161235355136 | b: 0.24424348458586545\n",
      " step548 | loss: -0.01556353801064759 | b: 0.24439911996597194\n",
      " step549 | loss: -0.015252267250435238 | b: 0.24455164263847629\n",
      " step550 | loss: -0.014947221905425466 | b: 0.24470111485753054\n",
      " step551 | loss: -0.014648277467317996 | b: 0.24484759763220373\n",
      " step552 | loss: -0.01435531191797196 | b: 0.24499115075138345\n",
      " step553 | loss: -0.014068205679613612 | b: 0.24513183280817957\n",
      " step554 | loss: -0.013786841566019349 | b: 0.24526970122383976\n",
      " step555 | loss: -0.013511104734698734 | b: 0.24540481227118674\n",
      " step556 | loss: -0.01324088264000494 | b: 0.2455372210975868\n",
      " step557 | loss: -0.012976064987207252 | b: 0.24566698174745888\n",
      " step558 | loss: -0.012716543687460558 | b: 0.24579414718433348\n",
      " step559 | loss: -0.012462212813711631 | b: 0.2459187693124706\n",
      " step560 | loss: -0.012212968557438444 | b: 0.24604089899804496\n",
      " step561 | loss: -0.011968709186287186 | b: 0.24616058608990785\n",
      " step562 | loss: -0.011729335002564624 | b: 0.24627787943993348\n",
      " step563 | loss: -0.011494748302511652 | b: 0.2463928269229586\n",
      " step564 | loss: -0.011264853336463574 | b: 0.24650547545632323\n",
      " step565 | loss: -0.011039556269731748 | b: 0.24661587101902055\n",
      " step566 | loss: -0.01081876514433901 | b: 0.24672405867046393\n",
      " step567 | loss: -0.010602389841450588 | b: 0.24683008256887842\n",
      " step568 | loss: -0.010390342044623111 | b: 0.24693398598932464\n",
      " step569 | loss: -0.010182535203729658 | b: 0.24703581134136193\n",
      " step570 | loss: -0.009978884499654157 | b: 0.24713560018635847\n",
      " step571 | loss: -0.009779306809662387 | b: 0.2472333932544551\n",
      " step572 | loss: -0.009583720673469855 | b: 0.2473292304611898\n",
      " step573 | loss: -0.009392046259998494 | b: 0.24742315092378978\n",
      " step574 | loss: -0.009204205334800122 | b: 0.24751519297713778\n",
      " step575 | loss: -0.009020121228103478 | b: 0.24760539418941882\n",
      " step576 | loss: -0.008839718803541246 | b: 0.24769379137745423\n",
      " step577 | loss: -0.008662924427468965 | b: 0.24778042062172892\n",
      " step578 | loss: -0.008489665938920013 | b: 0.24786531728111813\n",
      " step579 | loss: -0.008319872620142945 | b: 0.24794851600731957\n",
      " step580 | loss: -0.00815347516773997 | b: 0.24803005075899698\n",
      " step581 | loss: -0.007990405664382933 | b: 0.2481099548156408\n",
      " step582 | loss: -0.007830597551098464 | b: 0.24818826079115178\n",
      " step583 | loss: -0.007673985600074218 | b: 0.2482650006471525\n",
      " step584 | loss: -0.007520505888075632 | b: 0.24834020570603327\n",
      " step585 | loss: -0.00737009577031273 | b: 0.2484139066637364\n",
      " step586 | loss: -0.00722269385490705 | b: 0.24848613360228547\n",
      " step587 | loss: -0.00707823997780892 | b: 0.24855691600206356\n",
      " step588 | loss: -0.006936675178252046 | b: 0.24862628275384607\n",
      " step589 | loss: -0.006797941674685717 | b: 0.24869426217059293\n",
      " step590 | loss: -0.006661982841193463 | b: 0.24876088199900487\n",
      " step591 | loss: -0.0065287431843701425 | b: 0.24882616943084856\n",
      " step592 | loss: -0.006398168320681634 | b: 0.24889015111405538\n",
      " step593 | loss: -0.006270204954269474 | b: 0.24895285316359808\n",
      " step594 | loss: -0.006144800855181813 | b: 0.2490143011721499\n",
      " step595 | loss: -0.006021904838078455 | b: 0.24907452022053067\n",
      " step596 | loss: -0.005901466741316881 | b: 0.24913353488794385\n",
      " step597 | loss: -0.0057834374064910325 | b: 0.24919136926200874\n",
      " step598 | loss: -0.0056677686583633145 | b: 0.24924804694859237\n",
      " step599 | loss: -0.005554413285192084 | b: 0.2493035910814443\n",
      " step600 | loss: -0.005443325019492562 | b: 0.2493580243316392\n",
      " step601 | loss: -0.005334458519099883 | b: 0.2494113689168302\n",
      " step602 | loss: -0.005227769348717857 | b: 0.24946364661031736\n",
      " step603 | loss: -0.005123213961745848 | b: 0.24951487874993483\n",
      " step604 | loss: -0.005020749682509944 | b: 0.24956508624675994\n",
      " step605 | loss: -0.004920334688857793 | b: 0.2496142895936485\n",
      " step606 | loss: -0.004821927995081125 | b: 0.2496625088735993\n",
      " step607 | loss: -0.004725489435181629 | b: 0.24970976376795112\n",
      " step608 | loss: -0.004630979646476305 | b: 0.24975607356441587\n",
      " step609 | loss: -0.004538360053548161 | b: 0.24980145716495136\n",
      " step610 | loss: -0.004447592852476703 | b: 0.24984593309347614\n",
      " step611 | loss: -0.0043586409954278335 | b: 0.2498895195034304\n",
      " step612 | loss: -0.004271468175517725 | b: 0.2499322341851856\n",
      " step613 | loss: -0.004186038812008093 | b: 0.24997409457330566\n",
      " step614 | loss: -0.004102318035767496 | b: 0.25001511775366336\n",
      " step615 | loss: -0.0040202716750535215 | b: 0.2500553204704139\n",
      " step616 | loss: -0.003939866241551186 | b: 0.25009471913282944\n",
      " step617 | loss: -0.0038610689167197167 | b: 0.25013332982199665\n",
      " step618 | loss: -0.003783847538386027 | b: 0.2501711682973805\n",
      " step619 | loss: -0.0037081705876181557 | b: 0.2502082500032567\n",
      " step620 | loss: -0.0036340071758661453 | b: 0.25024459007501537\n",
      " step621 | loss: -0.003561327032348345 | b: 0.25028020334533885\n",
      " step622 | loss: -0.003490100491698911 | b: 0.25031510435025583\n",
      " step623 | loss: -0.0034202984818661264 | b: 0.2503493073350745\n",
      " step624 | loss: -0.0033518925122311316 | b: 0.2503828262601968\n",
      " step625 | loss: -0.0032848546619851505 | b: 0.25041567480681665\n",
      " step626 | loss: -0.0032191575687450327 | b: 0.2504478663825041\n",
      " step627 | loss: -0.0031547744173715843 | b: 0.25047941412667785\n",
      " step628 | loss: -0.0030916789290235157 | b: 0.2505103309159681\n",
      " step629 | loss: -0.003029845350443452 | b: 0.2505406293694725\n",
      " step630 | loss: -0.0029692484434339405 | b: 0.2505703218539069\n",
      " step631 | loss: -0.002909863474565526 | b: 0.2505994204886525\n",
      " step632 | loss: -0.0028516662050733998 | b: 0.25062793715070325\n",
      " step633 | loss: -0.00279463288097233 | b: 0.25065588347951295\n",
      " step634 | loss: -0.002738740223353773 | b: 0.25068327088174647\n",
      " step635 | loss: -0.0026839654188857763 | b: 0.25071011053593534\n",
      " step636 | loss: -0.002630286110509559 | b: 0.25073641339704045\n",
      " step637 | loss: -0.0025776803882980913 | b: 0.25076219020092344\n",
      " step638 | loss: -0.0025261267805318254 | b: 0.2507874514687288\n",
      " step639 | loss: -0.0024756042449214986 | b: 0.250812207511178\n",
      " step640 | loss: -0.0024260921600227905 | b: 0.25083646843277824\n",
      " step641 | loss: -0.0023775703168223573 | b: 0.2508602441359465\n",
      " step642 | loss: -0.002330018910484171 | b: 0.25088354432505133\n",
      " step643 | loss: -0.0022834185322764713 | b: 0.2509063785103741\n",
      " step644 | loss: -0.002237750161630032 | b: 0.25092875601199044\n",
      " step645 | loss: -0.0021929951583990713 | b: 0.25095068596357445\n",
      " step646 | loss: -0.0021491352552310164 | b: 0.25097217731612675\n",
      " step647 | loss: -0.0021061525501255575 | b: 0.250993238841628\n",
      " step648 | loss: -0.0020640294991230235 | b: 0.2510138791366192\n",
      " step649 | loss: -0.002022748909139693 | b: 0.25103410662571063\n",
      " step650 | loss: -0.0019822939309590026 | b: 0.2510539295650202\n",
      " step651 | loss: -0.001942648052339564 | b: 0.2510733560455436\n",
      " step652 | loss: -0.0019037950912903056 | b: 0.2510923939964565\n",
      " step653 | loss: -0.0018657191894661197 | b: 0.25111105118835114\n",
      " step654 | loss: -0.0018284048056771952 | b: 0.2511293352364079\n",
      " step655 | loss: -0.0017918367095644783 | b: 0.25114725360350354\n",
      " step656 | loss: -0.0017559999753711964 | b: 0.25116481360325726\n",
      " step657 | loss: -0.0017208799758640226 | b: 0.2511820224030159\n",
      " step658 | loss: -0.0016864623763488851 | b: 0.2511988870267794\n",
      " step659 | loss: -0.0016527331288203584 | b: 0.2512154143580676\n",
      " step660 | loss: -0.0016196784662437836 | b: 0.25123161114273\n",
      " step661 | loss: -0.0015872848969186748 | b: 0.2512474839916992\n",
      " step662 | loss: -0.0015555391989819612 | b: 0.25126303938368905\n",
      " step663 | loss: -0.0015244284150006137 | b: 0.25127828366783905\n",
      " step664 | loss: -0.0014939398467002719 | b: 0.2512932230663061\n",
      " step665 | loss: -0.0014640610497677641 | b: 0.25130786367680374\n",
      " step666 | loss: -0.0014347798287712976 | b: 0.2513222114750915\n",
      " step667 | loss: -0.0014060842321957524 | b: 0.25133627231741346\n",
      " step668 | loss: -0.0013779625475507373 | b: 0.25135005194288895\n",
      " step669 | loss: -0.0013504032966017122 | b: 0.25136355597585497\n",
      " step670 | loss: -0.0013233952306688935 | b: 0.25137678992816165\n",
      " step671 | loss: -0.0012969273260557657 | b: 0.2513897592014222\n",
      " step672 | loss: -0.0012709887795345765 | b: 0.25140246908921754\n",
      " step673 | loss: -0.001245569003945093 | b: 0.251414924779257\n",
      " step674 | loss: -0.0012206576238656909 | b: 0.25142713135549566\n",
      " step675 | loss: -0.0011962444713870468 | b: 0.2514390938002095\n",
      " step676 | loss: -0.001172319581961716 | b: 0.2514508169960291\n",
      " step677 | loss: -0.0011488731903216377 | b: 0.2514623057279323\n",
      " step678 | loss: -0.0011258957265127378 | b: 0.25147356468519744\n",
      " step679 | loss: -0.0011033778119823978 | b: 0.2514845984633173\n",
      " step680 | loss: -0.0010813102557452226 | b: 0.25149541156587474\n",
      " step681 | loss: -0.0010596840506296701 | b: 0.25150600840638104\n",
      " step682 | loss: -0.0010384903696179038 | b: 0.25151639331007725\n",
      " step683 | loss: -0.0010177205622254348 | b: 0.2515265705156995\n",
      " step684 | loss: -0.0009973661509788201 | b: 0.2515365441772093\n",
      " step685 | loss: -0.000977418827961003 | b: 0.2515463183654889\n",
      " step686 | loss: -0.000957870451401277 | b: 0.2515558970700029\n",
      " step687 | loss: -0.0009387130423732515 | b: 0.2515652842004267\n",
      " step688 | loss: -0.0009199387815257865 | b: 0.2515744835882419\n",
      " step689 | loss: -0.0009015400058945034 | b: 0.25158349898830085\n",
      " step690 | loss: -0.0008835092057775285 | b: 0.25159233408035864\n",
      " step691 | loss: -0.0008658390216615431 | b: 0.25160099247057527\n",
      " step692 | loss: -0.0008485222412281246 | b: 0.2516094776929875\n",
      " step693 | loss: -0.0008315517964034314 | b: 0.25161779321095157\n",
      " step694 | loss: -0.0008149207604759567 | b: 0.25162594241855635\n",
      " step695 | loss: -0.0007986223452665798 | b: 0.251633928642009\n",
      " step696 | loss: -0.0007826498983625641 | b: 0.25164175514099263\n",
      " step697 | loss: -0.0007669969003916322 | b: 0.25164942510999655\n",
      " step698 | loss: -0.0007516569623869884 | b: 0.25165694167962044\n",
      " step699 | loss: -0.0007366238231382738 | b: 0.2516643079178518\n",
      " step700 | loss: -0.0007218913466761023 | b: 0.2516715268313186\n",
      " step701 | loss: -0.0007074535197418186 | b: 0.25167860136651604\n",
      " step702 | loss: -0.0006933044493460728 | b: 0.2516855344110095\n",
      " step703 | loss: -0.0006794383603595122 | b: 0.25169232879461306\n",
      " step704 | loss: -0.000665849593155059 | b: 0.25169898729054463\n",
      " step705 | loss: -0.0006525326012891242 | b: 0.25170551261655755\n",
      " step706 | loss: -0.0006394819492636828 | b: 0.2517119074360502\n",
      " step707 | loss: -0.0006266923102782585 | b: 0.251718174359153\n",
      " step708 | loss: -0.0006141584640724318 | b: 0.25172431594379374\n",
      " step709 | loss: -0.0006018752947912276 | b: 0.25173033469674166\n",
      " step710 | loss: -0.0005898377888942719 | b: 0.2517362330746306\n",
      " step711 | loss: -0.0005780410331179553 | b: 0.2517420134849618\n",
      " step712 | loss: -0.0005664802124552182 | b: 0.25174767828708633\n",
      " step713 | loss: -0.000555150608205679 | b: 0.2517532297931684\n",
      " step714 | loss: -0.0005440475960448055 | b: 0.25175867026912885\n",
      " step715 | loss: -0.0005331666441213656 | b: 0.2517640019355701\n",
      " step716 | loss: -0.0005225033112394329 | b: 0.2517692269686825\n",
      " step717 | loss: -0.0005120532450132487 | b: 0.2517743475011326\n",
      " step718 | loss: -0.0005018121801138875 | b: 0.25177936562293374\n",
      " step719 | loss: -0.0004917759365096686 | b: 0.25178428338229886\n",
      " step720 | loss: -0.00048194041778060637 | b: 0.25178910278647665\n",
      " step721 | loss: -0.0004723016094276034 | b: 0.2517938258025709\n",
      " step722 | loss: -0.00046285557723564353 | b: 0.2517984543583433\n",
      " step723 | loss: -0.0004535984656922665 | b: 0.25180299034300024\n",
      " step724 | loss: -0.00044452649637719336 | b: 0.25180743560796404\n",
      " step725 | loss: -0.0004356359664497234 | b: 0.25181179196762854\n",
      " step726 | loss: -0.000426923247122204 | b: 0.25181606120009975\n",
      " step727 | loss: -0.00041838478217897545 | b: 0.25182024504792155\n",
      " step728 | loss: -0.0004100170865359587 | b: 0.25182434521878694\n",
      " step729 | loss: -0.00040181674480649576 | b: 0.251828363386235\n",
      " step730 | loss: -0.0003937804099081177 | b: 0.25183230119033406\n",
      " step731 | loss: -0.0003859048017123712 | b: 0.2518361602383512\n",
      " step732 | loss: -0.00037818670567702384 | b: 0.251839942105408\n",
      " step733 | loss: -0.0003706229715632503 | b: 0.2518436483351236\n",
      " step734 | loss: -0.00036321051212979684 | b: 0.2518472804402449\n",
      " step735 | loss: -0.00035594630188896304 | b: 0.2518508399032638\n",
      " step736 | loss: -0.00034882737585263614 | b: 0.2518543281770223\n",
      " step737 | loss: -0.0003418508283341737 | b: 0.25185774668530564\n",
      " step738 | loss: -0.0003350138117681922 | b: 0.25186109682342334\n",
      " step739 | loss: -0.00032831353553277156 | b: 0.25186437995877864\n",
      " step740 | loss: -0.00032174726482139706 | b: 0.25186759743142684\n",
      " step741 | loss: -0.00031531231952385496 | b: 0.2518707505546221\n",
      " step742 | loss: -0.00030900607313370186 | b: 0.25187384061535345\n",
      " step743 | loss: -0.0003028259516723608 | b: 0.25187686887487015\n",
      " step744 | loss: -0.00029676943263694965 | b: 0.25187983656919655\n",
      " step745 | loss: -0.0002908340439871893 | b: 0.2518827449096364\n",
      " step746 | loss: -0.00028501736310616367 | b: 0.25188559508326747\n",
      " step747 | loss: -0.00027931701584563487 | b: 0.2518883882534259\n",
      " step748 | loss: -0.0002737306755271618 | b: 0.2518911255601812\n",
      " step749 | loss: -0.0002682560620178265 | b: 0.25189380812080137\n",
      " step750 | loss: -0.0002628909407758329 | b: 0.25189643703020914\n",
      " step751 | loss: -0.00025763312196005474 | b: 0.2518990133614287\n",
      " step752 | loss: -0.00025248045952153575 | b: 0.25190153816602395\n",
      " step753 | loss: -0.00024743085033136937 | b: 0.25190401247452726\n",
      " step754 | loss: -0.0002424822333260579 | b: 0.2519064372968605\n",
      " step755 | loss: -0.0002376325886585562 | b: 0.2519088136227471\n",
      " step756 | loss: -0.00023287993688370535 | b: 0.25191114242211593\n",
      " step757 | loss: -0.0002282223381462245 | b: 0.2519134246454974\n",
      " step758 | loss: -0.00022365789138575564 | b: 0.2519156612244112\n",
      " step759 | loss: -0.00021918473355626134 | b: 0.2519178530717468\n",
      " step760 | loss: -0.00021480103888720238 | b: 0.2519200010821357\n",
      " step761 | loss: -0.0002105050181073409 | b: 0.25192210613231675\n",
      " step762 | loss: -0.0002062949177452822 | b: 0.2519241690814942\n",
      " step763 | loss: -0.0002021690193919312 | b: 0.2519261907716881\n",
      " step764 | loss: -0.00019812563900273972 | b: 0.2519281720280781\n",
      " step765 | loss: -0.00019416312622311694 | b: 0.2519301136593403\n",
      " step766 | loss: -0.00019027986369735573 | b: 0.2519320164579773\n",
      " step767 | loss: -0.00018647426642331766 | b: 0.25193388120064153\n",
      " step768 | loss: -0.00018274478109418625 | b: 0.2519357086484525\n",
      " step769 | loss: -0.00017908988547333138 | b: 0.2519374995473072\n",
      " step770 | loss: -0.000175508087764058 | b: 0.25193925462818484\n",
      " step771 | loss: -0.00017199792601090282 | b: 0.25194097460744497\n",
      " step772 | loss: -0.00016855796748828312 | b: 0.25194266018711986\n",
      " step773 | loss: -0.00016518680813959463 | b: 0.25194431205520124\n",
      " step774 | loss: -0.00016188307197651853 | b: 0.251945930885921\n",
      " step775 | loss: -0.00015864541053616676 | b: 0.25194751734002635\n",
      " step776 | loss: -0.00015547250232401667 | b: 0.2519490720650496\n",
      " step777 | loss: -0.00015236305228000902 | b: 0.2519505956955724\n",
      " step778 | loss: -0.00014931579123299345 | b: 0.2519520888534847\n",
      " step779 | loss: -0.00014632947540917484 | b: 0.2519535521482388\n",
      " step780 | loss: -0.0001434028859014802 | b: 0.2519549861770978\n",
      " step781 | loss: -0.00014053482818511044 | b: 0.25195639152537963\n",
      " step782 | loss: -0.00013772413161888152 | b: 0.25195776876669584\n",
      " step783 | loss: -0.00013496964898664034 | b: 0.2519591184631857\n",
      " step784 | loss: -0.00013227025600755837 | b: 0.2519604411657458\n",
      " step785 | loss: -0.00012962485088820585 | b: 0.25196173741425465\n",
      " step786 | loss: -0.0001270323538690832 | b: 0.25196300773779334\n",
      " step787 | loss: -0.00012449170679346367 | b: 0.2519642526548613\n",
      " step788 | loss: -0.00012200187265761997 | b: 0.2519654726735879\n",
      " step789 | loss: -0.00011956183520140939 | b: 0.25196666829193987\n",
      " step790 | loss: -0.00011717059849928547 | b: 0.2519678399979249\n",
      " step791 | loss: -0.00011482718653027746 | b: 0.2519689882697902\n",
      " step792 | loss: -0.00011253064279856062 | b: 0.2519701135762182\n",
      " step793 | loss: -0.00011028002994265763 | b: 0.2519712163765176\n",
      " step794 | loss: -0.0001080744293433611 | b: 0.25197229712081104\n",
      " step795 | loss: -0.00010591294075652513 | b: 0.2519733562502186\n",
      " step796 | loss: -0.00010379468194273045 | b: 0.25197439419703804\n",
      " step797 | loss: -0.00010171878830391279 | b: 0.2519754113849211\n",
      " step798 | loss: -9.96844125370444e-05 | b: 0.25197640822904643\n",
      " step799 | loss: -9.769072428440495e-05 | b: 0.25197738513628926\n",
      " step800 | loss: -9.573690980005267e-05 | b: 0.25197834250538725\n",
      " step801 | loss: -9.382217160350592e-05 | b: 0.2519792807271033\n",
      " step802 | loss: -9.194572817250446e-05 | b: 0.251980200184385\n",
      " step803 | loss: -9.010681361004913e-05 | b: 0.2519811012525211\n",
      " step804 | loss: -8.830467733673686e-05 | b: 0.25198198429929447\n",
      " step805 | loss: -8.653858379034318e-05 | b: 0.25198284968513235\n",
      " step806 | loss: -8.480781211375189e-05 | b: 0.25198369776325347\n",
      " step807 | loss: -8.311165587230107e-05 | b: 0.2519845288798122\n",
      " step808 | loss: -8.144942275407629e-05 | b: 0.25198534337403977\n",
      " step809 | loss: -7.982043430146746e-05 | b: 0.2519861415783828\n",
      " step810 | loss: -7.822402561345144e-05 | b: 0.2519869238186389\n",
      " step811 | loss: -7.665954510144957e-05 | b: 0.2519876904140899\n",
      " step812 | loss: -7.512635419899994e-05 | b: 0.2519884416776319\n",
      " step813 | loss: -7.362382711491478e-05 | b: 0.25198917791590303\n",
      " step814 | loss: -7.21513505739324e-05 | b: 0.25198989942940875\n",
      " step815 | loss: -7.070832356149026e-05 | b: 0.2519906065126444\n",
      " step816 | loss: -6.929415708938792e-05 | b: 0.2519912994542153\n",
      " step817 | loss: -6.790827394780764e-05 | b: 0.25199197853695476\n",
      " step818 | loss: -6.655010846984055e-05 | b: 0.25199264403803945\n",
      " step819 | loss: -6.521910629857075e-05 | b: 0.25199329622910244\n",
      " step820 | loss: -6.391472417433874e-05 | b: 0.2519939353763442\n",
      " step821 | loss: -6.263642968889372e-05 | b: 0.25199456174064105\n",
      " step822 | loss: -6.138370109766811e-05 | b: 0.25199517557765205\n",
      " step823 | loss: -6.015602707478252e-05 | b: 0.2519957771379228\n",
      " step824 | loss: -5.8952906535836294e-05 | b: 0.25199636666698816\n",
      " step825 | loss: -5.7773848404707453e-05 | b: 0.2519969444054722\n",
      " step826 | loss: -5.6618371434637996e-05 | b: 0.25199751058918657\n",
      " step827 | loss: -5.5486004005018685e-05 | b: 0.2519980654492266\n",
      " step828 | loss: -5.4376283925705594e-05 | b: 0.25199860921206585\n",
      " step829 | loss: -5.328875824702095e-05 | b: 0.2519991420996483\n",
      " step830 | loss: -5.222298308126483e-05 | b: 0.2519996643294791\n",
      " step831 | loss: -5.117852342053198e-05 | b: 0.2520001761147133\n",
      " step832 | loss: -5.015495295296546e-05 | b: 0.25200067766424283\n",
      " step833 | loss: -4.915185389407384e-05 | b: 0.25200116918278176\n",
      " step834 | loss: -4.816881681449559e-05 | b: 0.2520016508709499\n",
      " step835 | loss: -4.720544047927433e-05 | b: 0.2520021229253547\n",
      " step836 | loss: -4.626133166979685e-05 | b: 0.2520025855386714\n",
      " step837 | loss: -4.5336105035715945e-05 | b: 0.25200303889972175\n",
      " step838 | loss: -4.4429382934225715e-05 | b: 0.2520034831935511\n",
      " step839 | loss: -4.3540795278147474e-05 | b: 0.2520039186015039\n",
      " step840 | loss: -4.266997937278916e-05 | b: 0.2520043453012976\n",
      " step841 | loss: -4.1816579783926504e-05 | b: 0.25200476346709544\n",
      " step842 | loss: -4.098024818659951e-05 | b: 0.25200517326957733\n",
      " step843 | loss: -4.016064322314605e-05 | b: 0.25200557487600955\n",
      " step844 | loss: -3.9357430361093295e-05 | b: 0.25200596845031314\n",
      " step845 | loss: -3.8570281753749216e-05 | b: 0.25200635415313066\n",
      " step846 | loss: -3.7798876116994504e-05 | b: 0.25200673214189184\n",
      " step847 | loss: -3.7042898593142585e-05 | b: 0.2520071025708778\n",
      " step848 | loss: -3.6302040622189226e-05 | b: 0.252007465591284\n",
      " step849 | loss: -3.557599981135695e-05 | b: 0.25200782135128214\n",
      " step850 | loss: -3.486447981373431e-05 | b: 0.2520081699960803\n",
      " step851 | loss: -3.416719021842596e-05 | b: 0.25200851166798244\n",
      " step852 | loss: -3.3483846416260124e-05 | b: 0.2520088465064466\n",
      " step853 | loss: -3.281416948610172e-05 | b: 0.2520091746481415\n",
      " step854 | loss: -3.215788609580273e-05 | b: 0.25200949622700247\n",
      " step855 | loss: -3.151472837288338e-05 | b: 0.2520098113742862\n",
      " step856 | loss: -3.0884433806903646e-05 | b: 0.25201012021862423\n",
      " step857 | loss: -3.0266745130518303e-05 | b: 0.25201042288607556\n",
      " step858 | loss: -2.966141022824331e-05 | b: 0.25201071950017784\n",
      " step859 | loss: -2.9068182024474256e-05 | b: 0.25201101018199806\n",
      " step860 | loss: -2.8486818383157697e-05 | b: 0.2520112950501819\n",
      " step861 | loss: -2.7917082016557517e-05 | b: 0.25201157422100207\n",
      " step862 | loss: -2.735874037611552e-05 | b: 0.25201184780840585\n",
      " step863 | loss: -2.6811565569317963e-05 | b: 0.25201211592406153\n",
      " step864 | loss: -2.627533425751949e-05 | b: 0.2520123786774041\n",
      " step865 | loss: -2.5749827570109575e-05 | b: 0.2520126361756798\n",
      " step866 | loss: -2.5234831021379023e-05 | b: 0.25201288852399\n",
      " step867 | loss: -2.4730134398680548e-05 | b: 0.252013135825334\n",
      " step868 | loss: -2.4235531709848603e-05 | b: 0.2520133781806511\n",
      " step869 | loss: -2.375082107647586e-05 | b: 0.2520136156888619\n",
      " step870 | loss: -2.3275804655185083e-05 | b: 0.25201384844690844\n",
      " step871 | loss: -2.281028856302214e-05 | b: 0.2520140765497941\n",
      " step872 | loss: -2.2354082790343455e-05 | b: 0.252014300090622\n",
      " step873 | loss: -2.190700113558819e-05 | b: 0.25201451916063333\n",
      " step874 | loss: -2.1468861113618232e-05 | b: 0.25201473384924444\n",
      " step875 | loss: -2.1039483891200915e-05 | b: 0.2520149442440833\n",
      " step876 | loss: -2.0618694212828358e-05 | b: 0.25201515043102546\n",
      " step877 | loss: -2.0206320328526318e-05 | b: 0.25201535249422874\n",
      " step878 | loss: -1.9802193922089373e-05 | b: 0.252015550516168\n",
      " step879 | loss: -1.9406150041731962e-05 | b: 0.2520157445776684\n",
      " step880 | loss: -1.901802704196598e-05 | b: 0.2520159347579388\n",
      " step881 | loss: -1.863766650103571e-05 | b: 0.2520161211346038\n",
      " step882 | loss: -1.8264913173737795e-05 | b: 0.2520163037837356\n",
      " step883 | loss: -1.7899614908003513e-05 | b: 0.25201648277988464\n",
      " step884 | loss: -1.7541622610082187e-05 | b: 0.25201665819611074\n",
      " step885 | loss: -1.7190790156007552e-05 | b: 0.2520168301040123\n",
      " step886 | loss: -1.6846974356070634e-05 | b: 0.25201699857375587\n",
      " step887 | loss: -1.651003486770719e-05 | b: 0.25201716367410454\n",
      " step888 | loss: -1.6179834170628736e-05 | b: 0.25201732547244626\n",
      " step889 | loss: -1.58562374865312e-05 | b: 0.2520174840348211\n",
      " step890 | loss: -1.553911273646236e-05 | b: 0.25201763942594846\n",
      " step891 | loss: -1.5228330482415231e-05 | b: 0.25201779170925326\n",
      " step892 | loss: -1.4923763872047858e-05 | b: 0.252017940946892\n",
      " step893 | loss: -1.4625288595055964e-05 | b: 0.252018087199778\n",
      " step894 | loss: -1.4332782822066292e-05 | b: 0.2520182305276062\n",
      " step895 | loss: -1.404612716598308e-05 | b: 0.25201837098887786\n",
      " step896 | loss: -1.3765204622302463e-05 | b: 0.25201850864092407\n",
      " step897 | loss: -1.348990053173793e-05 | b: 0.25201864353992937\n",
      " step898 | loss: -1.3220102520961064e-05 | b: 0.25201877574095455\n",
      " step899 | loss: -1.2955700470058673e-05 | b: 0.25201890529795923\n",
      " step900 | loss: -1.2696586461373727e-05 | b: 0.25201903226382383\n",
      " step901 | loss: -1.2442654731188442e-05 | b: 0.2520191566903711\n",
      " step902 | loss: -1.2193801636755098e-05 | b: 0.25201927862838747\n",
      " step903 | loss: -1.1949925602863233e-05 | b: 0.2520193981276435\n",
      " step904 | loss: -1.1710927092138945e-05 | b: 0.25201951523691446\n",
      " step905 | loss: -1.1476708550191006e-05 | b: 0.25201963000399996\n",
      " step906 | loss: -1.1247174379462876e-05 | b: 0.25201974247574377\n",
      " step907 | loss: -1.1022230889068397e-05 | b: 0.25201985269805266\n",
      " step908 | loss: -1.08017862734755e-05 | b: 0.25201996071591537\n",
      " step909 | loss: -1.058575054884159e-05 | b: 0.25202006657342085\n",
      " step910 | loss: -1.0374035535960502e-05 | b: 0.2520201703137762\n",
      " step911 | loss: -1.0166554826440688e-05 | b: 0.2520202719793245\n",
      " step912 | loss: -9.96322372799341e-06 | b: 0.25202037161156177\n",
      " step913 | loss: -9.763959255337795e-06 | b: 0.25202046925115434\n",
      " step914 | loss: -9.568680070088931e-06 | b: 0.25202056493795505\n",
      " step915 | loss: -9.377306468678626e-06 | b: 0.25202065871101975\n",
      " step916 | loss: -9.189760341854481e-06 | b: 0.25202075060862317\n",
      " step917 | loss: -9.005965133752625e-06 | b: 0.2520208406682745\n",
      " step918 | loss: -8.825845829534274e-06 | b: 0.25202092892673283\n",
      " step919 | loss: -8.649328913605814e-06 | b: 0.252021015420022\n",
      " step920 | loss: -8.476342334375885e-06 | b: 0.2520211001834453\n",
      " step921 | loss: -8.306815488197117e-06 | b: 0.25202118325160017\n",
      " step922 | loss: -8.140679179433619e-06 | b: 0.252021264658392\n",
      " step923 | loss: -7.977865595591994e-06 | b: 0.25202134443704793\n",
      " step924 | loss: -7.818308283447095e-06 | b: 0.2520214226201308\n",
      " step925 | loss: -7.661942116783394e-06 | b: 0.2520214992395519\n",
      " step926 | loss: -7.508703275647122e-06 | b: 0.2520215743265847\n",
      " step927 | loss: -7.358529209966491e-06 | b: 0.25202164791187676\n",
      " step928 | loss: -7.21135862590927e-06 | b: 0.252021720025463\n",
      " step929 | loss: -7.067131452203057e-06 | b: 0.25202179069677755\n",
      " step930 | loss: -6.9257888237928004e-06 | b: 0.2520218599546658\n",
      " step931 | loss: -6.78727304602944e-06 | b: 0.25202192782739624\n",
      " step932 | loss: -6.6515275872802705e-06 | b: 0.25202199434267214\n",
      " step933 | loss: -6.518497035585824e-06 | b: 0.2520220595276425\n",
      " step934 | loss: -6.38812709368608e-06 | b: 0.25202212340891345\n",
      " step935 | loss: -6.260364551593512e-06 | b: 0.25202218601255894\n",
      " step936 | loss: -6.135157260445112e-06 | b: 0.25202224736413154\n",
      " step937 | loss: -6.0124541163020245e-06 | b: 0.2520223074886727\n",
      " step938 | loss: -5.89220503329102e-06 | b: 0.25202236641072306\n",
      " step939 | loss: -5.774360932804256e-06 | b: 0.2520224241543324\n",
      " step940 | loss: -5.658873712945933e-06 | b: 0.25202248074306954\n",
      " step941 | loss: -5.545696240005782e-06 | b: 0.25202253620003195\n",
      " step942 | loss: -5.434782317337295e-06 | b: 0.25202259054785514\n",
      " step943 | loss: -5.326086669583674e-06 | b: 0.25202264380872186\n",
      " step944 | loss: -5.219564935003973e-06 | b: 0.2520226960043712\n",
      " step945 | loss: -5.115173635488191e-06 | b: 0.25202274715610756\n",
      " step946 | loss: -5.012870165046479e-06 | b: 0.2520227972848092\n",
      " step947 | loss: -4.912612760819002e-06 | b: 0.2520228464109368\n",
      " step948 | loss: -4.8143605042128e-06 | b: 0.25202289455454185\n",
      " step949 | loss: -4.718073294043279e-06 | b: 0.2520229417352748\n",
      " step950 | loss: -4.623711828912747e-06 | b: 0.2520229879723931\n",
      " step951 | loss: -4.531237591720583e-06 | b: 0.252023033284769\n",
      " step952 | loss: -4.440612840710401e-06 | b: 0.2520230776908974\n",
      " step953 | loss: -4.351800584174725e-06 | b: 0.25202312120890324\n",
      " step954 | loss: -4.26476457306535e-06 | b: 0.252023163856549\n",
      " step955 | loss: -4.179469280956027e-06 | b: 0.2520232056512418\n",
      " step956 | loss: -4.095879896652832e-06 | b: 0.2520232466100408\n",
      " step957 | loss: -4.013962297051421e-06 | b: 0.25202328674966373\n",
      " step958 | loss: -3.933683051400294e-06 | b: 0.25202332608649425\n",
      " step959 | loss: -3.855009390889563e-06 | b: 0.25202336463658814\n",
      " step960 | loss: -3.7779092033929376e-06 | b: 0.25202340241568016\n",
      " step961 | loss: -3.702351018546324e-06 | b: 0.25202343943919037\n",
      " step962 | loss: -3.6283039969475793e-06 | b: 0.25202347572223033\n",
      " step963 | loss: -3.555737918787827e-06 | b: 0.25202351127960954\n",
      " step964 | loss: -3.484623159693001e-06 | b: 0.25202354612584116\n",
      " step965 | loss: -3.4149306968345173e-06 | b: 0.25202358027514815\n",
      " step966 | loss: -3.3466320826391893e-06 | b: 0.252023613741469\n",
      " step967 | loss: -3.2796994403838654e-06 | b: 0.2520236465384634\n",
      " step968 | loss: -3.2141054521162004e-06 | b: 0.2520236786795179\n",
      " step969 | loss: -3.149823342312175e-06 | b: 0.2520237101777513\n",
      " step970 | loss: -3.08682687517603e-06 | b: 0.2520237410460201\n",
      " step971 | loss: -3.0250903395767636e-06 | b: 0.2520237712969235\n",
      " step972 | loss: -2.964588532279322e-06 | b: 0.2520238009428088\n",
      " step973 | loss: -2.9052967623499625e-06 | b: 0.25202382999577644\n",
      " step974 | loss: -2.8471908237293067e-06 | b: 0.25202385846768466\n",
      " step975 | loss: -2.790247009727409e-06 | b: 0.25202388637015477\n",
      " step976 | loss: -2.7344420708175223e-06 | b: 0.2520239137145755\n",
      " step977 | loss: -2.679753226999537e-06 | b: 0.25202394051210775\n",
      " step978 | loss: -2.6261581633946207e-06 | b: 0.2520239667736894\n",
      " step979 | loss: -2.5736350002603103e-06 | b: 0.2520239925100394\n",
      " step980 | loss: -2.5221622995275083e-06 | b: 0.2520240177316624\n",
      " step981 | loss: -2.4717190527212554e-06 | b: 0.2520240424488529\n",
      " step982 | loss: -2.4222846745658444e-06 | b: 0.25202406667169963\n",
      " step983 | loss: -2.373838980105347e-06 | b: 0.25202409041008944\n",
      " step984 | loss: -2.3263622013303122e-06 | b: 0.25202411367371147\n",
      " step985 | loss: -2.279834954777016e-06 | b: 0.252024136472061\n",
      " step986 | loss: -2.234238257017296e-06 | b: 0.25202415881444357\n",
      " step987 | loss: -2.1895534926841265e-06 | b: 0.2520241807099785\n",
      " step988 | loss: -2.145762422003372e-06 | b: 0.25202420216760274\n",
      " step989 | loss: -2.1028471721251665e-06 | b: 0.2520242231960745\n",
      " step990 | loss: -2.0607902318658943e-06 | b: 0.2520242438039768\n",
      " step991 | loss: -2.0195744252760052e-06 | b: 0.25202426399972105\n",
      " step992 | loss: -1.9791829357984625e-06 | b: 0.2520242837915504\n",
      " step993 | loss: -1.939599279836557e-06 | b: 0.2520243031875432\n",
      " step994 | loss: -1.9008072922588326e-06 | b: 0.25202432219561616\n",
      " step995 | loss: -1.862791146294285e-06 | b: 0.2520243408235276\n",
      " step996 | loss: -1.8255353245422158e-06 | b: 0.2520243590788808\n",
      " step997 | loss: -1.7890246168406066e-06 | b: 0.252024376969127\n",
      " step998 | loss: -1.7532441249556996e-06 | b: 0.25202439450156827\n",
      " step999 | loss: -1.71817924282891e-06 | b: 0.2520244116833607\n",
      "0.2520244116833607\n"
     ]
    }
   ],
   "source": [
    "gd = GDRegressor(0.01, 1000)\n",
    "gd.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c7d29c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
